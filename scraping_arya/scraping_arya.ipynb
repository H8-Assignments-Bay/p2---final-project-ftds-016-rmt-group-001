{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from dateutil import parser\n",
    "from random import randint\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol TRGU date_elements 161 dates 160 comment_elements 161 comments 160 \n",
      "symbol CAMP date_elements 101 dates 100 comment_elements 101 comments 100 \n",
      "symbol WMUU date_elements 241 dates 240 comment_elements 261 comments 240 \n",
      "symbol MAIN date_elements 101 dates 100 comment_elements 101 comments 100 \n",
      "symbol CEKA date_elements 41 dates 40 comment_elements 41 comments 40 \n",
      "symbol CSRA date_elements 121 dates 120 comment_elements 121 comments 120 \n",
      "symbol AISA date_elements 441 dates 440 comment_elements 441 comments 440 \n",
      "symbol HOKI date_elements 221 dates 220 comment_elements 221 comments 220 \n"
     ]
    }
   ],
   "source": [
    "symbols = [\"TRGU\", \"CAMP\", \"WMUU\", \"MAIN\", \"CEKA\", \"CSRA\", \"AISA\", \"HOKI\"]\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for symbol in symbols:\n",
    "    driver.get(f\"https://stockbit.com/symbol/{symbol}\")\n",
    "    \n",
    "    # Handdle: IndexError: list index out of range\n",
    "    while True:\n",
    "        date_elements = driver.find_elements(by=By.XPATH, value=\"//div[@class='stream-date']\")\n",
    "        \n",
    "        # Handle: IndexError: list index out of range\n",
    "        if len(date_elements) != 0:\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "        # date_elements[-2] Handle: StaleElementReferenceException or ''\n",
    "        # .text[:16] Handle: ParserError: Unknown string format: 14 Dec 22, 19:42 Â· Edited on 14 Dec 22, 21:29\n",
    "        if parser.parse(date_elements[-2].text[:16]) >= parser.parse(\"1 Oct 22\"):\n",
    "            driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\", \"\")\n",
    "            date_elements = driver.find_elements(by=By.XPATH, value=\"//div[@class='stream-date']\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # value='stream-text-single' Handle: prevent Repost to be included in the comment_elements    \n",
    "    comment_elements = driver.find_elements(by=By.CLASS_NAME, value=\"stream-text-single\")\n",
    "    \n",
    "    dates = list()\n",
    "    comments = list()\n",
    "    \n",
    "    for index, (date_element, comment_element) in enumerate(zip(date_elements, comment_elements)):\n",
    "        # print(date_element.text, comment_element.text, \"\\n\")\n",
    "        \n",
    "        # Handle: StaleElementReferenceException or ''\n",
    "        if index == len(date_elements) - 1:\n",
    "            break\n",
    "        \n",
    "        dates.append(date_element.text)\n",
    "        comments.append(comment_element.text)\n",
    "        \n",
    "    print(f'symbol {symbol} '\n",
    "          f'date_elements {len(date_elements)} dates {len(dates)} '\n",
    "          f'comment_elements {len(comment_elements)} comments {len(comments)} ')\n",
    "    \n",
    "    df_c = pd.DataFrame()\n",
    "    df_c['date'] = dates\n",
    "    df_c['comment'] = comments\n",
    "    df_c['symbol'] = symbol\n",
    "    df_c.drop(len(df_c) -1)\n",
    "    \n",
    "    df = pd.concat([df, df_c])\n",
    "    \n",
    "    sleep(randint(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scraping_arya.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e968b3e28a8eff326c0713b24a2703acb5b4eecbe24764f446bb5f80254562b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
