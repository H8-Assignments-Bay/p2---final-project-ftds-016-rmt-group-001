{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jason/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dong pola kadal purba'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_slangwords_stopwords():\n",
    "    global slangwords, stopwords\n",
    "    \n",
    "    with open('./slangwords.txt', 'rt') as file_1:\n",
    "        slangwords = {}\n",
    "        for line in file_1:\n",
    "            (key, val) = line.rstrip().split(\":\")\n",
    "            slangwords[key] = val\n",
    "\n",
    "    with open('./stopwords_v2.txt') as file_2:\n",
    "        stopwords = [item.rstrip() for item in list(file_2)]\n",
    "\n",
    "load_slangwords_stopwords()        \n",
    "\n",
    "def preprocess(comment: str):\n",
    "  \n",
    "  # Menghilangkan nama saham\n",
    "  comment = re.sub(\"\\$[A-Z]{4}| [A-Z]{4} \", \" \", comment)\n",
    "\n",
    "  # Mengubah text ke Lowercase agar semua data seragam\n",
    "  comment = comment.lower()\n",
    "\n",
    "  # Menghilangkan @/Mention karena pada berita palsu ada mention akun twitter\n",
    "  comment = re.sub(\"@[A-Za-z0-9]+\", \" \", comment)\n",
    "\n",
    "  # Menghilangkan #/Hashtag untuk mengantisipasi karena berita palsu mengambil dari twitter\n",
    "  comment = re.sub(\"#[A-Za-z0-9_]+\", \" \", comment)\n",
    "  \n",
    "  # Menghilangkan \\n untuk antisipasi\n",
    "  comment = re.sub(r\"\\n\", \" \",comment)\n",
    "\n",
    "  # Menghilangkan Whitespace untuk antisipasi\n",
    "  comment = comment.strip()\n",
    "\n",
    "  # Menghilangkan Link dikarenakan berita palsu terdapat link ke artikel lain\n",
    "  comment = re.sub(r\"http\\S+\", \" \", comment)\n",
    "  comment = re.sub(r\"www.\\S+\", \" \", comment)\n",
    "\n",
    "  # Menghilangkan yang Bukan Huruf seperti Emoji, Simbol Matematika (seperti μ), dst untuk antisipasi\n",
    "  comment = re.sub(\"[^A-Za-z\\s']\", \" \", comment)\n",
    "\n",
    "  # Melakukan Tokenisasi\n",
    "  tokens = word_tokenize(comment)\n",
    "\n",
    "  # Menghilangkan Stopwords\n",
    "  comment = ' '.join([word for word in tokens if word not in stopwords])\n",
    "\n",
    "  # Melakukan lemmatizing\n",
    "  comment = \" \".join(slangwords.get(word, word) for word in comment.split())\n",
    "  \n",
    "  return comment\n",
    "\n",
    "preprocess('@TakaTin sudah ada dong pola kadal purba di $INDF\\n\\n-------\\n$MYOR $ICBP $GOOD $LPPF\\n\\n⬇️⬇️⬇️\\nhttps://stockbit.com/post/8338381')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('../ICBP_MLBI_labeled.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['comment'], df['label 2'])\n",
    "isinstance(X_train, pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = joblib.load('./count_vect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<136x5020 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5087 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(comment: pd.Series):\n",
    "    return vectorizer.transform(X_train)\n",
    "\n",
    "transform(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
